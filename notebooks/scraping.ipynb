{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8c01c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from random import random\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "434a699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url):\n",
    "    time.sleep(0.3 * random())\n",
    "    r = requests.get(url)\n",
    "    return r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1701dc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_finals_data():\n",
    "    url = 'https://en.wikipedia.org/wiki/List_of_Grand_Slam_singles_finals'\n",
    "    html_text = get_html(url)\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    \n",
    "    for it, table in enumerate(soup.find_all('table', class_ = 'sortable')):\n",
    "        table = table.tbody\n",
    "        \n",
    "        data = []\n",
    "        for row in table.find_all('tr'):\n",
    "            crs = row.find_all('td')\n",
    "            data.append((crs[0].get_text().strip(),\n",
    "                         crs[1].get_text().strip(),\n",
    "                         crs[2].get_text().strip(),\n",
    "                         crs[3].get_text().strip(),\n",
    "                         crs[4].get_text().strip()))\n",
    "\n",
    "        df = pd.DataFrame(data[1:], columns = data[0])\n",
    "        df.to_csv('data/' + ('wo' if it == 1 else '') + 'men_finals.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c8f498e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_birth_date(x):\n",
    "    try:\n",
    "        return x.split(')')[0].split('(')[1]\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "\n",
    "def parse_height(x):\n",
    "    try:\n",
    "        x = x.split('(')\n",
    "        ret = ''\n",
    "        if 'in' in x[0]:\n",
    "            ret = x[1].split(')')[0]\n",
    "        else:\n",
    "            ret = x[0]\n",
    "        ret = ret.strip()\n",
    "        return ret\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "\n",
    "def get_players_info(gender):\n",
    "    df = pd.read_csv('data/%s_finals.csv' % gender)\n",
    "    players = set(df['Winner']) | set(df['Runner-up'])\n",
    "\n",
    "    data = []\n",
    "    for it, player in enumerate(players):\n",
    "        if (it + 1) % 10 == 0:\n",
    "            print('%3d/%3d' % (it + 1, len(players)))\n",
    "        url = 'https://en.wikipedia.org/wiki/' + player\n",
    "        html_text = get_html(url)\n",
    "        if 'Wikipedia does not have an article with this exact name.' in html_text:\n",
    "            print('Skipping: %s' % player)\n",
    "            continue\n",
    "        soup = BeautifulSoup(html_text, 'html.parser')\n",
    "\n",
    "        cr = {'Name': player}\n",
    "        for infobox in soup.find_all('table', class_ = 'infobox vcard'):\n",
    "            for row in infobox.find_all('tr')[1:]:\n",
    "                if len(row.find_all('th', class_ = 'infobox-header')) > 0:\n",
    "                    break\n",
    "                try:\n",
    "                    attr = unicodedata.normalize('NFKD', row.find_all('th')[0].get_text().strip())\n",
    "                    val = unicodedata.normalize('NFKD', row.find_all('td')[0].get_text().strip())\n",
    "                    cr[attr] = val\n",
    "                except:\n",
    "                    pass\n",
    "        data.append(cr)\n",
    "        \n",
    "        image = soup.find_all('td', class_ = 'infobox-image')\n",
    "        if len(image) == 0:\n",
    "            continue\n",
    "        image_url = 'https:' + image[0].find_all('a')[0].img.attrs['src']\n",
    "        with open('data/player_images/%s.jpg' % player, 'wb') as fw:\n",
    "            fw.write(requests.get(image_url).content)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df['Born'] = df['Born'].apply(parse_birth_date)\n",
    "    df['Height'] = df['Height'].apply(parse_height)\n",
    "    df.to_csv('data/%s_players_info.csv' % gender, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c175060b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping: Reginald F. Doherty\n",
      " 10/294\n",
      "Skipping: Geoff E. Brown\n",
      " 20/294\n",
      "Skipping: C. Gene Mako\n",
      "Skipping: Francis Kovacs, 2d\n",
      " 30/294\n",
      "Skipping: E. Victor Seixas, Jr.\n",
      "Skipping: Francis T. Hunter\n",
      " 40/294\n",
      "Skipping: George M. Lott, Jr.\n",
      "Skipping: C.St.John\n",
      " 50/294\n",
      "Skipping: Tom P. Brown\n",
      "Skipping: Cecil Parke\n",
      " 60/294\n",
      "Skipping: E. Victor Seixas Jr.\n",
      "Skipping: Henry W. Slocum Jr.\n",
      " 70/294\n",
      "Skipping: Wilmer L. Allison\n",
      "Skipping: S. Welby van Horn\n",
      "Skipping: Giorgo de Stefani\n",
      " 80/294\n",
      "Skipping: R. Falkenburg\n",
      "Skipping: Henry W. Slocum, Jr.\n",
      "Skipping: Frederick R. Schroeder, Jr.\n",
      " 90/294\n",
      "100/294\n",
      "Skipping: E. Pockley\n",
      "110/294\n",
      "Skipping: Hugh L. Doherty\n",
      "Skipping: Fred H. Hovey\n",
      "Skipping: Robert D. Wrenn\n",
      "120/294\n",
      "Skipping: Thomas C. Bundy\n",
      "130/294\n",
      "Skipping: C.R. McKinley\n",
      "140/294\n",
      "Skipping: H. Roper Barrett\n",
      "150/294\n",
      "Skipping: Richard N. Williams\n",
      "160/294\n",
      "Skipping: Francis X. Shields\n",
      "170/294\n",
      "Skipping: E.F. Parker\n",
      "Skipping: V. St. Leger Gould\n",
      "Skipping: Roderik Menzel\n",
      "180/294\n",
      "Skipping: F.R. Schroeder\n",
      "Skipping: William A. Larned\n",
      "190/294\n",
      "Skipping: Ashley J. Cooper\n",
      "200/294\n",
      "Skipping: Lt. Joseph R. Hunt\n",
      "Skipping: William F. Talbert\n",
      "Skipping: Richard A. Gonzales\n",
      "210/294\n",
      "Skipping: R.Cummings\n",
      "220/294\n",
      "Skipping: A. Curtis\n",
      "Skipping: Malcolm J. Anderson\n",
      "230/294\n",
      "Skipping: Pat O\\'Hara Wood\n",
      "Skipping: S.B. Wood\n",
      "240/294\n",
      "250/294\n",
      "Skipping: A.R.F. Kingscote\n",
      "260/294\n",
      "Skipping: A. Beamish\n",
      "Skipping: Frank Froehling, III\n",
      "270/294\n",
      "280/294\n",
      "290/294\n",
      "Skipping: Margorie Crawford\n",
      " 10/252\n",
      "Skipping: Hilde Kranwinkel\n",
      "Skipping: Eileen Bennett Whitingstall\n",
      " 20/252\n",
      " 30/252\n",
      " 40/252\n",
      "Skipping: A. Morton\n",
      "Skipping: Bertha L. Townsend\n",
      " 50/252\n",
      "Skipping: Katherine Le Messurier\n",
      " 60/252\n",
      "Skipping: Hilde Kranwinkel Sperling\n",
      " 70/252\n",
      "Skipping: Carolin A. Babcock\n",
      " 80/252\n",
      "Skipping: Darlene R. Hard\n",
      " 90/252\n",
      "100/252\n",
      "110/252\n",
      "120/252\n",
      "130/252\n",
      "Skipping: Helen J. Jacobs\n",
      "140/252\n",
      "150/252\n",
      "Skipping: Thelma Long-Coyne\n",
      "160/252\n",
      "170/252\n",
      "Skipping: E. Bennett\n",
      "180/252\n",
      "190/252\n",
      "Skipping: Lida D. Voorhes\n",
      "Skipping: Molla B. Mallory\n",
      "200/252\n",
      "210/252\n",
      "220/252\n",
      "230/252\n",
      "Skipping: Ellen C Roosevelt\n",
      "240/252\n",
      "250/252\n"
     ]
    }
   ],
   "source": [
    "# get_finals_data()\n",
    "get_players_info('men')\n",
    "get_players_info('women')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f20572f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_match_stats():\n",
    "    data_list = []\n",
    "    tournament_event_ids = []\n",
    "    tournament_ids = [17, 18, 19, 21]\n",
    "    for slam in tournament_ids:\n",
    "        page = 1\n",
    "        while True:\n",
    "            url = 'https://www.ultimatetennisstatistics.com/tournamentEventsTable?tournamentId=%d' %slam + '&current=%d' %page\n",
    "            html_text = json.loads(get_html(url))\n",
    "            tournament_event_ids.extend([d['id'] for d in html_text['rows']])\n",
    "            page += 1\n",
    "            if html_text['rowCount'] == 0: break\n",
    "\n",
    "    for tournament_event_id in tournament_event_ids:\n",
    "        url = 'https://www.ultimatetennisstatistics.com/tournamentEvent?tournamentEventId=%d' % tournament_event_id\n",
    "        html_text = get_html(url)\n",
    "        soup = BeautifulSoup(html_text, 'html.parser')\n",
    "\n",
    "        data = {\n",
    "            'Tournament': soup.find_all('h3')[0].get_text().strip()\n",
    "        }\n",
    "\n",
    "        fields = [(2, 2), (3, 1), (4, 0), (4, 1), (4, 2)]\n",
    "        for col, tr in fields:    \n",
    "            cr = soup.find_all('div', class_ = 'col-md-%d' % col)[0].find_all('tr')[tr].get_text().strip().split('\\n')\n",
    "            data[cr[0]] = cr[-1]\n",
    "        data['Winner'] = ' '.join(data['Winner'].split()[:-1])\n",
    "        data['Runner-up'] = ' '.join(data['Runner-up'].split()[:-1])\n",
    "\n",
    "        try:\n",
    "            match_id = int(soup.find_all('table', class_ = 'table-condensed text-nowrap')[0].tbody.find_all('tr')[0].find_all('td', attrs = {'data-round-index': True})[6].find_all('a', attrs = {'onclick': True})[0].attrs['id'].split('-')[1])\n",
    "            url = 'https://www.ultimatetennisstatistics.com/matchStats?matchId=%d' % match_id\n",
    "            html_text = get_html(url)\n",
    "            soup = BeautifulSoup(html_text, 'html.parser')\n",
    "        except:\n",
    "            match_id = None\n",
    "\n",
    "        if match_id is None:\n",
    "            print('Skipping: %s' % data['Tournament'])\n",
    "            continue\n",
    "\n",
    "        for stats in soup.find_all('div', class_ = 'tab-content')[0].find_all('div'):\n",
    "            stats_type = stats.attrs['id'].split('%s' % match_id)[1]\n",
    "\n",
    "            subtype = ''\n",
    "            for substats in stats.find_all('table')[0].find_all('tr'):\n",
    "                if len(substats.find_all('i')) > 0:\n",
    "                    subtype = substats.find_all('th')[2].get_text().strip()\n",
    "                    continue\n",
    "\n",
    "                ths = substats.find_all('th')\n",
    "\n",
    "                try:\n",
    "                    subsubtype = substats.find_all('td')[0].get_text().strip()\n",
    "                    data['%s_%s_%s_Winner' % (stats_type, subtype, subsubtype)] = ths[1].get_text().strip()\n",
    "                    data['%s_%s_%s_Runner-up' % (stats_type, subtype, subsubtype)] = ths[2].get_text().strip()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        data_list.append(data)\n",
    "\n",
    "    df = pd.DataFrame(data_list)\n",
    "    df.to_csv('data/men_match_stats.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7e9d31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping: Roland Garros 1990\n",
      "Skipping: Roland Garros 1989\n",
      "Skipping: Roland Garros 1988\n",
      "Skipping: Roland Garros 1987\n",
      "Skipping: Roland Garros 1986\n",
      "Skipping: Roland Garros 1985\n",
      "Skipping: Roland Garros 1984\n",
      "Skipping: Roland Garros 1983\n",
      "Skipping: Roland Garros 1982\n",
      "Skipping: Roland Garros 1981\n",
      "Skipping: Roland Garros 1980\n",
      "Skipping: Roland Garros 1979\n",
      "Skipping: Roland Garros 1978\n",
      "Skipping: Roland Garros 1977\n",
      "Skipping: Roland Garros 1976\n",
      "Skipping: Roland Garros 1975\n",
      "Skipping: Roland Garros 1974\n",
      "Skipping: Roland Garros 1973\n",
      "Skipping: Roland Garros 1972\n",
      "Skipping: Roland Garros 1971\n",
      "Skipping: Roland Garros 1970\n",
      "Skipping: Roland Garros 1969\n",
      "Skipping: Roland Garros 1968\n",
      "Skipping: Wimbledon 1990\n",
      "Skipping: Wimbledon 1989\n",
      "Skipping: Wimbledon 1988\n",
      "Skipping: Wimbledon 1987\n",
      "Skipping: Wimbledon 1986\n",
      "Skipping: Wimbledon 1985\n",
      "Skipping: Wimbledon 1984\n",
      "Skipping: Wimbledon 1983\n",
      "Skipping: Wimbledon 1982\n",
      "Skipping: Wimbledon 1981\n",
      "Skipping: Wimbledon 1980\n",
      "Skipping: Wimbledon 1979\n",
      "Skipping: Wimbledon 1978\n",
      "Skipping: Wimbledon 1977\n",
      "Skipping: Wimbledon 1976\n",
      "Skipping: Wimbledon 1975\n",
      "Skipping: Wimbledon 1974\n",
      "Skipping: Wimbledon 1973\n",
      "Skipping: Wimbledon 1972\n",
      "Skipping: Wimbledon 1971\n",
      "Skipping: Wimbledon 1970\n",
      "Skipping: Wimbledon 1969\n",
      "Skipping: Wimbledon 1968\n",
      "Skipping: US Open 1996\n",
      "Skipping: US Open 1990\n",
      "Skipping: US Open 1989\n",
      "Skipping: US Open 1988\n",
      "Skipping: US Open 1987\n",
      "Skipping: US Open 1986\n",
      "Skipping: US Open 1985\n",
      "Skipping: US Open 1984\n",
      "Skipping: US Open 1983\n",
      "Skipping: US Open 1982\n",
      "Skipping: US Open 1981\n",
      "Skipping: US Open 1980\n",
      "Skipping: US Open 1979\n",
      "Skipping: US Open 1978\n",
      "Skipping: US Open 1977\n",
      "Skipping: US Open 1976\n",
      "Skipping: US Open 1975\n",
      "Skipping: US Open 1974\n",
      "Skipping: US Open 1973\n",
      "Skipping: US Open 1972\n",
      "Skipping: US Open 1971\n",
      "Skipping: US Open 1970\n",
      "Skipping: US Open 1969\n",
      "Skipping: US Open 1968\n",
      "Skipping: Australian Open 1993\n",
      "Skipping: Australian Open 1990\n",
      "Skipping: Australian Open 1989\n",
      "Skipping: Australian Open 1988\n",
      "Skipping: Australian Open 1987\n",
      "Skipping: Australian Open 1985\n",
      "Skipping: Australian Open 1984\n",
      "Skipping: Australian Open 1983\n",
      "Skipping: Australian Open 1982\n",
      "Skipping: Australian Open 1981\n",
      "Skipping: Australian Open 1980\n",
      "Skipping: Australian Open 1979\n",
      "Skipping: Australian Open 1978\n",
      "Skipping: Australian Open-2 1977\n",
      "Skipping: Australian Open 1977\n",
      "Skipping: Australian Open 1976\n",
      "Skipping: Australian Open 1975\n",
      "Skipping: Australian Open 1974\n",
      "Skipping: Australian Open 1973\n",
      "Skipping: Australian Open 1972\n",
      "Skipping: Australian Open 1971\n",
      "Skipping: Australian Open 1970\n",
      "Skipping: Australian Open 1969\n",
      "Skipping: Australian Chps. 1968\n"
     ]
    }
   ],
   "source": [
    "get_match_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d341d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
